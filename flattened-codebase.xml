<?xml version="1.0" encoding="UTF-8"?>
<files>
	<file path='.bmadignore'>
		# data í´ë” ì „ì²´ ì œì™¸
		data/
		
		# ë¡œê·¸ íŒŒì¼ ì œì™¸
		*.log
		
		# ìºì‹œ/ì„ì‹œ íŒŒì¼ ì œì™¸
		*.tmp
		*.cache</file>
	<file path='.github/workflows/soop.yml'><![CDATA[
		name: soop-collector
		
		on:
		  schedule:
		    - cron: "0 * * * *"      # ë§¤ì‹œ ì •ê° (ì¹´í…Œê³ ë¦¬)
		    - cron: "0 0,12 * * *"   # í•˜ë£¨ 2íšŒ (ìƒì„¸) - UTC 00,12ì‹œ = KST 09,21ì‹œ
		  workflow_dispatch: {}
		
		permissions:
		  contents: write
		
		# ê°™ì€ ì›Œí¬í”Œë¡œê°€ ê²¹ì¹˜ë©´ ì´ì „ ì‹¤í–‰ ì·¨ì†Œ â†’ push ë ˆì´ìŠ¤ ê°ì†Œ
		concurrency:
		  group: soop-collector
		  cancel-in-progress: true
		
		jobs:
		  categories_hourly:
		    runs-on: ubuntu-latest
		    steps:
		      # rebaseê°€ ê°€ëŠ¥í•˜ë„ë¡ ì „ì²´ ì´ë ¥ ì²´í¬ì•„ì›ƒ
		      - uses: actions/checkout@v4
		        with:
		          fetch-depth: 0
		
		      - uses: actions/setup-python@v5
		        with:
		          python-version: "3.11"
		          cache: "pip"
		
		      - name: Install deps
		        run: |
		          python -m pip install --upgrade pip
		          pip install -r requirements.txt
		
		      - name: Run category snapshot
		        run: |
		          python collect_categories.py
		
		      - name: Commit & push (rebase-safe)
		        shell: bash
		        run: |
		          git config user.name "github-actions[bot]"
		          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
		
		          # ë³€ê²½ ì—†ìœ¼ë©´ ì¢…ë£Œ
		          if git diff --quiet && git diff --cached --quiet; then
		            echo "no changes"; exit 0
		          fi
		
		          git add -A
		          git commit -m "snapshot: $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || true
		
		          # ì›ê²© ìµœì‹  ì´ë ¥ ë°˜ì˜ í›„ í‘¸ì‹œ (ì§§ê²Œ ì¬ì‹œë„)
		          for i in {1..3}; do
		            git fetch origin main
		            git rebase origin/main || { git rebase --abort; git pull --rebase origin main; }
		            git push origin HEAD:main && break || { echo "push race, retry $i"; sleep 3; }
		          done
		
		  details_twice_daily:
		    runs-on: ubuntu-latest
		    steps:
		      - uses: actions/checkout@v4
		        with:
		          fetch-depth: 0
		
		      - uses: actions/setup-python@v5
		        with:
		          python-version: "3.11"
		          cache: "pip"
		
		      - name: Install deps
		        run: |
		          python -m pip install --upgrade pip
		          pip install -r requirements.txt
		
		      - name: Run details snapshot
		        run: |
		          python collect_details.py
		
		      - name: Commit & push (rebase-safe)
		        shell: bash
		        run: |
		          git config user.name "github-actions[bot]"
		          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
		
		          if git diff --quiet && git diff --cached --quiet; then
		            echo "no changes"; exit 0
		          fi
		
		          git add -A
		          git commit -m "details: $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || true
		
		          for i in {1..3}; do
		            git fetch origin main
		            git rebase origin/main || { git rebase --abort; git pull --rebase origin main; }
		            git push origin HEAD:main && break || { echo "push race, retry $i"; sleep 3; }
		          done]]></file>
	<file path='.gitignore'>
		venv/
		__pycache__/
		.vscode/</file>
	<file path='analyze_categories.py'>
		import pandas as pd
		from pathlib import Path
		
		DATA_FILE = Path("data/soop/categories/categories_master.csv")
		
		def load_data():
		    return pd.read_csv(DATA_FILE)
		
		def analyze():
		    df = load_data()
		    # ê¸°ë³¸ íƒ€ì… ìºìŠ¤íŒ…
		    df["view_cnt"] = pd.to_numeric(df["view_cnt"], errors="coerce").fillna(0).astype(int)
		
		    # ìµœê·¼ ì‹œê°ë³„ í”¼ë²— (ì¹´í…Œê³ ë¦¬ë³„ ì‹œì²­ì í•©)
		    pivot = df.pivot_table(index="captured_at_utc",
		                           columns="category_name",
		                           values="view_cnt",
		                           aggfunc="sum")
		    pivot.to_csv("data/soop/categories/pivot_timeseries.csv", encoding="utf-8-sig")
		
		    # ìµœê·¼ ìŠ¤ëƒ…ìƒ· ìƒìœ„ ì¹´í…Œê³ ë¦¬
		    latest_time = df["captured_at_utc"].max()
		    latest = df[df["captured_at_utc"] == latest_time].sort_values("view_cnt", ascending=False)
		    latest.to_csv("data/soop/categories/top_latest.csv", index=False, encoding="utf-8-sig")
		
		    print("Saved pivot_timeseries.csv and top_latest.csv")
		
		if __name__ == "__main__":
		    analyze()</file>
	<file path='collect_categories.py'>
		# collect_categories.py
		# -*- coding: utf-8 -*-
		"""
		SOOP ì¹´í…Œê³ ë¦¬ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ (CSV ì €ì¥, ë¼ë²¨ë§/ê°€ê³µ ì—†ìŒ)
		- categoryList APIë¥¼ í˜ì´ì§• ëê¹Œì§€ ìˆ˜ì§‘
		- ì›ë³¸ í•„ë“œë§Œ ì €ì¥: category_no, category_name, view_cnt, fixed_tags, cate_img, captured_at_utc, platform
		- ì €ì¥:
		  1) ì‹œê°ë³„ ìŠ¤ëƒ…ìƒ·: data/soop/categories/YYYY/MM/DD/HH.csv
		  2) ë§ˆìŠ¤í„° ëˆ„ì :   data/soop/categories/categories_master.csv
		  3) ì‹œê³„ì—´ ëˆ„ì :   data/soop/categories_timeseries.csv  â† ì´ë²ˆì— ì¶”ê°€
		"""
		
		import time
		import pathlib
		from datetime import datetime, timezone
		from typing import List, Tuple, Dict, Any
		
		import requests
		import pandas as pd
		
		# ======================
		# ì„¤ì •
		# ======================
		BASE = "https://sch.sooplive.co.kr/api.php"
		HEADERS = {
		    "User-Agent": "Mozilla/5.0",
		    "Accept": "application/json, text/plain, */*",
		    "Referer": "https://www.sooplive.co.kr/",
		}
		PAGE_SIZE = 120
		ORDER = "view_cnt"
		SLEEP_BETWEEN_PAGES = 0.35
		
		OUT_ROOT = pathlib.Path("data/soop/categories")
		OUT_ROOT.mkdir(parents=True, exist_ok=True)
		MASTER_CSV = OUT_ROOT / "categories_master.csv"
		TIMESERIES_CSV = pathlib.Path("data/soop/categories_timeseries.csv")
		
		# ======================
		# ìš”ì²­/ìˆ˜ì§‘
		# ======================
		def fetch_category_page(page_no: int, n_per: int = PAGE_SIZE, order: str = ORDER) -> Tuple[List[Dict[str, Any]], bool]:
		    params = {
		        "m": "categoryList",
		        "szKeyword": "",
		        "szOrder": order,
		        "nPageNo": page_no,
		        "nListCnt": n_per,
		        "nOffset": 0,
		        "szPlatform": "pc",
		    }
		    backoff = 1.0
		    for attempt in range(5):
		        try:
		            r = requests.get(BASE, params=params, headers=HEADERS, timeout=15)
		            r.raise_for_status()
		            j = r.json()
		            data = j.get("data", {})
		            items = data.get("list", []) or []
		            is_more = bool(data.get("is_more", False))
		            return items, is_more
		        except Exception:
		            if attempt == 4:
		                raise
		            time.sleep(backoff)
		            backoff = min(backoff * 2, 10)
		    return [], False
		
		def fetch_all_categories() -> pd.DataFrame:
		    rows: List[Dict[str, Any]] = []
		    page = 1
		    while True:
		        items, is_more = fetch_category_page(page)
		        rows += items
		        if not is_more:
		            break
		        page += 1
		        time.sleep(SLEEP_BETWEEN_PAGES)
		    if not rows:
		        return pd.DataFrame(columns=["category_no","category_name","view_cnt","fixed_tags","cate_img"])
		    df = pd.DataFrame(rows)
		    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ (ì›ë³¸ ìœ ì§€)
		    keep = [c for c in ["category_no","category_name","view_cnt","fixed_tags","cate_img"] if c in df.columns]
		    df = df[keep].copy()
		    # ë©”íƒ€
		    ts = datetime.now(timezone.utc).replace(microsecond=0).isoformat()
		    df["captured_at_utc"] = ts
		    df["platform"] = "soop"
		    return df
		
		def save_snapshot_csv(df: pd.DataFrame) -> pathlib.Path:
		    ts = pd.to_datetime(df["captured_at_utc"].iloc[0])
		    outdir = OUT_ROOT / f"{ts.year:04d}" / f"{ts.month:02d}" / f"{ts.day:02d}"
		    outdir.mkdir(parents=True, exist_ok=True)
		    outpath = outdir / f"{ts.hour:02d}.csv"
		    df.to_csv(outpath, index=False, encoding="utf-8-sig")
		    return outpath
		
		def append_master_csv(df: pd.DataFrame) -> pathlib.Path:
		    header = not MASTER_CSV.exists()
		    df.to_csv(MASTER_CSV, index=False, mode="a", header=header, encoding="utf-8-sig")
		    return MASTER_CSV
		
		def upsert_timeseries_csv(df: pd.DataFrame) -> pathlib.Path:
		    """ì‹œê³„ì—´ ëˆ„ì  íŒŒì¼ ê°±ì‹  (append í›„ ê°™ì€ ì‹œê°/ì¹´í…Œê³ ë¦¬ ì¤‘ë³µ ì œê±°)"""
		    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶•ì†Œ
		    ts_part = df[["captured_at_utc","category_no","category_name","view_cnt"]].copy()
		    # ì •ì‹œ ë²„í‚· ë³´ì •(ì•ˆì „)
		    ts_part["captured_at_utc"] = pd.to_datetime(ts_part["captured_at_utc"], utc=True).dt.floor("H")
		
		    if TIMESERIES_CSV.exists():
		        old = pd.read_csv(TIMESERIES_CSV, encoding="utf-8-sig")
		        # íƒ€ì… ë³´ì •
		        if "captured_at_utc" in old.columns:
		            old["captured_at_utc"] = pd.to_datetime(old["captured_at_utc"], utc=True).dt.floor("H")
		        ts_all = pd.concat([old, ts_part], ignore_index=True)
		    else:
		        TIMESERIES_CSV.parent.mkdir(parents=True, exist_ok=True)
		        ts_all = ts_part
		
		    # ê°™ì€ ì‹œê°(captured_at_utc) + ê°™ì€ ì¹´í…Œê³ ë¦¬(category_no) ì¤‘ë³µ ì œê±° (ë§ˆì§€ë§‰ ê°’ ìœ ì§€)
		    ts_all = ts_all.drop_duplicates(["captured_at_utc","category_no"], keep="last")
		    ts_all.sort_values(["captured_at_utc","category_no"], inplace=True)
		    ts_all.to_csv(TIMESERIES_CSV, index=False, encoding="utf-8-sig")
		    return TIMESERIES_CSV
		
		def main():
		    df_all = fetch_all_categories()
		    if df_all.empty:
		        print("ë¹ˆ ì‘ë‹µ. ì ì‹œ í›„ ì¬ì‹œë„ ë°”ëŒ.")
		        return
		
		    # ì½˜ì†” í™•ì¸(ìƒìœ„ 25)
		    if "view_cnt" in df_all.columns:
		        df_sorted = df_all.sort_values("view_cnt", ascending=False).copy()
		        cols_show = [c for c in ["category_no","category_name","view_cnt","fixed_tags"] if c in df_sorted.columns]
		        print("\n=== Snapshot (top 25 by view_cnt) ===")
		        print(df_sorted[cols_show].head(25).to_string(index=False))
		        print(f"\nrows_total={len(df_all)}")
		
		    snap = save_snapshot_csv(df_all)
		    master = append_master_csv(df_all)
		    tsfile = upsert_timeseries_csv(df_all)
		    print(f"\nsaved snapshot -> {snap}")
		    print(f"appended master -> {master}")
		    print(f"updated timeseries -> {tsfile}")
		
		if __name__ == "__main__":
		    main()</file>
	<file path='collect_details.py'>
		import requests
		import pandas as pd
		from datetime import datetime, timezone
		import pathlib
		
		BASE = "https://sch.sooplive.co.kr/api.php"
		HEADERS = {"User-Agent": "Mozilla/5.0"}
		DATA_DIR = pathlib.Path("data/soop/details")
		DATA_DIR.mkdir(parents=True, exist_ok=True)
		
		# ì›í•˜ëŠ” category_no ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì§€ì •
		category_nos = ["00810000", "00040070"]  #ë²„ì¶”ì–¼, FCì˜¨ë¼ì¸
		
		def fetch_category_contents(cate_no, page=1, nListCnt=60):
		    params = {
		        "m": "categoryContentsList",
		        "szType": "live",
		        "nPageNo": page,
		        "nListCnt": nListCnt,
		        "szPlatform": "pc",
		        "szOrder": "view_cnt_desc",
		        "szCateNo": cate_no,
		    }
		    r = requests.get(BASE, params=params, headers=HEADERS, timeout=15)
		    r.raise_for_status()
		    js = r.json()
		    return js["data"]["list"], js["data"]["is_more"]
		
		def fetch_all_for_category(cate_no):
		    all_items = []
		    page = 1
		    while True:
		        items, is_more = fetch_category_contents(cate_no, page)
		        all_items.extend(items)
		        if not is_more:
		            break
		        page += 1
		    return all_items
		
		def save_snapshot(df: pd.DataFrame, cate_no: str):
		    now = datetime.now(timezone.utc)
		    df["captured_at_utc"] = now.isoformat()
		    df["platform"] = "soop"
		    df["category_no"] = cate_no
		
		    # ì‹œê°ë³„ ìŠ¤ëƒ…ìƒ·
		    snapshot_dir = DATA_DIR / cate_no / now.strftime("%Y/%m/%d")
		    snapshot_dir.mkdir(parents=True, exist_ok=True)
		    snapshot_file = snapshot_dir / f"{now.strftime('%H')}.csv"
		    df.to_csv(snapshot_file, index=False, encoding="utf-8-sig")
		
		    # ëˆ„ì  ë§ˆìŠ¤í„°
		    master_file = DATA_DIR / "details_master.csv"
		    if master_file.exists():
		        old = pd.read_csv(master_file)
		        df = pd.concat([old, df], ignore_index=True)
		    df.to_csv(master_file, index=False, encoding="utf-8-sig")
		    print(f"Saved {len(df)} rows for category {cate_no} to {snapshot_file}")
		
		def main():
		    for cate_no in category_nos:
		        items = fetch_all_for_category(cate_no)
		        if not items:
		            continue
		        df = pd.DataFrame(items)
		        cols = ["broad_no", "broad_title", "user_id", "user_nick",
		                "view_cnt", "broad_start", "hash_tags"]
		        df = df[cols]
		        save_snapshot(df, cate_no)
		
		if __name__ == "__main__":
		    main()
		
		# # -*- coding: utf-8 -*-
		# """
		# SOOP ì¹´í…Œê³ ë¦¬ ìƒì„¸(ë°©ì†¡ ë‹¨ìœ„) ìˆ˜ì§‘ (CSV ì €ì¥)
		# - ì…ë ¥: ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ ëª©ë¡(category_nos)
		# - ë™ì‘: ê° cate_noì— ëŒ€í•´ categoryContentsListë¥¼ í˜ì´ì§• ëê¹Œì§€ ìˆ˜ì§‘
		# - ì¶œë ¥:
		#   1) ì‹œê°ë³„ ìƒì„¸ ìŠ¤ëƒ…ìƒ· CSV: data/soop/details/YYYY/MM/DD/HH.csv
		#   2) ë§ˆìŠ¤í„° ëˆ„ì  CSV:        data/soop/details/details_master.csv
		# """
		
		# import os
		# import time
		# import json
		# import pathlib
		# from datetime import datetime, timezone
		# from typing import List, Tuple, Dict, Any
		
		# import requests
		# import pandas as pd
		
		# # ======================
		# # ì„¤ì •
		# # ======================
		# BASE = "https://sch.sooplive.co.kr/api.php"
		# HEADERS = {
		#     "User-Agent": "Mozilla/5.0",
		#     "Accept": "application/json, text/plain, */*",
		#     "Referer": "https://www.sooplive.co.kr/",
		# }
		# PAGE_SIZE = 60
		# ORDER = "view_cnt_desc"
		# SLEEP_BETWEEN_PAGES = 0.3
		# SLEEP_BETWEEN_CATS = 0.8
		
		# OUT_ROOT = pathlib.Path("data/soop/details")
		# MASTER_CSV = OUT_ROOT / "details_master.csv"
		
		# # ìˆ˜ì§‘ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ (ì˜ˆì‹œ: ë¡œìŠ¤íŠ¸ì•„í¬, ë²„ì¶”ì–¼, ë¡¤, ë¡¤í† ì²´ìŠ¤ ë“±)
		# category_nos = [
		#     # "00040067",  # ë¡œìŠ¤íŠ¸ì•„í¬
		#     # "00810000",  # ë²„ì¶”ì–¼
		#     # "00040019",  # ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ
		#     # "00040075",  # ì „ëµì  íŒ€ ì „íˆ¬(TFT)
		# ]
		
		# # ======================
		# # ìš”ì²­/ìˆ˜ì§‘ í•¨ìˆ˜
		# # ======================
		# def fetch_contents_page(cate_no: str, page_no: int, n_per: int = PAGE_SIZE, order: str = ORDER) -> Tuple[List[Dict[str, Any]], bool]:
		#     params = {
		#         "m": "categoryContentsList",
		#         "szType": "live",
		#         "nPageNo": page_no,
		#         "nListCnt": n_per,
		#         "szPlatform": "pc",
		#         "szOrder": order,
		#         "szCateNo": cate_no,
		#     }
		#     backoff = 1.0
		#     for attempt in range(5):
		#         try:
		#             r = requests.get(BASE, params=params, headers=HEADERS, timeout=15)
		#             r.raise_for_status()
		#             j = r.json()
		#             data = j.get("data", {})
		#             items = data.get("list", []) or []
		#             is_more = bool(data.get("is_more", False))
		#             return items, is_more
		#         except Exception:
		#             if attempt == 4:
		#                 raise
		#             time.sleep(backoff)
		#             backoff = min(backoff * 2, 10)
		#     return [], False
		
		# def fetch_category_details(cate_no: str) -> pd.DataFrame:
		#     """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë°©ì†¡ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ìˆ˜ì§‘"""
		#     rows: List[Dict[str, Any]] = []
		#     page = 1
		#     while True:
		#         items, is_more = fetch_contents_page(cate_no, page)
		#         rows += items
		#         if not is_more:
		#             break
		#         page += 1
		#         time.sleep(SLEEP_BETWEEN_PAGES)
		#     if not rows:
		#         return pd.DataFrame(columns=[
		#             "broad_no","broad_title","user_id","user_nick","view_cnt",
		#             "broad_start","hash_tags","thumbnail","user_profile_img",
		#             "category_no","captured_at_utc","platform"
		#         ])
		#     df = pd.DataFrame(rows)
		
		#     # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
		#     keep = [c for c in [
		#         "broad_no","broad_title","user_id","user_nick","view_cnt",
		#         "broad_start","hash_tags","thumbnail","user_profile_img"
		#     ] if c in df.columns]
		#     df = df[keep].copy()
		
		#     # ë©”íƒ€ ì¶”ê°€
		#     ts = datetime.now(timezone.utc).replace(microsecond=0).isoformat()
		#     df["captured_at_utc"] = ts
		#     df["platform"] = "soop"
		#     df["category_no"] = cate_no
		#     return df
		
		# def fetch_multi_categories(cate_list: List[str]) -> pd.DataFrame:
		#     frames = []
		#     for cate in cate_list:
		#         df = fetch_category_details(cate)
		#         if not df.empty:
		#             frames.append(df)
		#         time.sleep(SLEEP_BETWEEN_CATS)
		#     return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
		
		# def save_snapshot_csv(df: pd.DataFrame) -> pathlib.Path:
		#     ts = pd.to_datetime(df["captured_at_utc"].iloc[0])
		#     outdir = OUT_ROOT / f"{ts.year:04d}" / f"{ts.month:02d}" / f"{ts.day:02d}"
		#     outdir.mkdir(parents=True, exist_ok=True)
		#     outpath = outdir / f"{ts.hour:02d}.csv"
		#     df.to_csv(outpath, index=False, encoding="utf-8")
		#     return outpath
		
		# def append_master_csv(df: pd.DataFrame) -> pathlib.Path:
		#     OUT_ROOT.mkdir(parents=True, exist_ok=True)
		#     header = not MASTER_CSV.exists()
		#     df.to_csv(MASTER_CSV, index=False, mode="a", header=header, encoding="utf-8")
		#     return MASTER_CSV
		
		# def main():
		#     if not category_nos:
		#         print("âš ï¸ category_nos ë¦¬ìŠ¤íŠ¸ì— ìˆ˜ì§‘í•  cate_noë¥¼ í•˜ë‚˜ ì´ìƒ ë„£ì–´ì£¼ì„¸ìš”.")
		#         return
		
		#     df_all = fetch_multi_categories(category_nos)
		#     if df_all.empty:
		#         print("ìˆ˜ì§‘ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
		#         return
		
		#     # ì½˜ì†”ì— ìƒìœ„ 20ê°œ ì¶œë ¥
		#     cols_show = [c for c in ["category_no","broad_no","user_id","user_nick","view_cnt","broad_title"] if c in df_all.columns]
		#     print("\n=== Details snapshot (top 20 by view_cnt) ===")
		#     print(df_all.sort_values("view_cnt", ascending=False)[cols_show].head(20).to_string(index=False))
		
		#     # ì €ì¥
		#     snap = save_snapshot_csv(df_all)
		#     master = append_master_csv(df_all)
		#     snap_kb = snap.stat().st_size/1024 if snap.exists() else 0
		#     mast_kb = master.stat().st_size/1024 if master.exists() else 0
		#     print(f"\nsaved snapshot -> {snap} ({snap_kb:.1f} KB)")
		#     print(f"appended master -> {master} ({mast_kb:.1f} KB total)")
		
		# if __name__ == "__main__":
		#     main()</file>
	<file path='flattened-codebase.stats.md'><![CDATA[
		# ğŸ§¾ Flatten Stats for flattened-codebase.xml
		
		## ğŸ“Š Summary
		- Total source size: 17.9 MB
		- Generated XML size: 21.1 MB
		- Total lines of code: 162,399
		- Estimated tokens: 5,520,308
		- File breakdown: 128 text, 0 binary, 0 errors
		
		## ğŸ“ˆ Size Percentiles
		Avg: 146,421 B, Median: 66,894 B, p90: 66,942 B, p95: 66,965 B, p99: 2,259,956 B
		
		## ğŸ§® Size Histogram
		| Bucket | Files | Bytes |
		| --- | ---: | ---: |
		| 0â€“1KB | 5 | 1,139 |
		| 1â€“10KB | 3 | 15,346 |
		| 10â€“100KB | 118 | 7,840,567 |
		| 100KBâ€“1MB | 0 | 0 |
		| 1â€“10MB | 2 | 10,884,798 |
		| 10â€“100MB | 0 | 0 |
		| >=100MB | 0 | 0 |
		
		## ğŸ“¦ Top Extensions by Bytes (Top 20)
		| Ext | Files | Bytes | % of total |
		| --- | ---: | ---: | ---: |
		| .csv | 119 | 18,714,953 | 99.86% |
		| .py | 3 | 13,583 | 0.07% |
		| .md | 1 | 10,412 | 0.06% |
		| .yml | 1 | 2,730 | 0.01% |
		| <none> | 2 | 96 | 0.00% |
		| .json | 1 | 60 | 0.00% |
		| .txt | 1 | 16 | 0.00% |
		
		## ğŸ“‚ Top Directories by Bytes (Top 20)
		| Directory | Files | Bytes | % of total |
		| --- | ---: | ---: | ---: |
		| data | 119 | 18,714,953 | 99.86% |
		| data/soop | 119 | 18,714,953 | 99.86% |
		| data/soop/categories | 118 | 16,454,997 | 87.80% |
		| data/soop/categories/2025 | 117 | 7,830,155 | 41.78% |
		| data/soop/categories/2025/08 | 117 | 7,830,155 | 41.78% |
		| data/soop/categories/2025/08/24 | 22 | 1,471,718 | 7.85% |
		| data/soop/categories/2025/08/22 | 22 | 1,471,711 | 7.85% |
		| data/soop/categories/2025/08/21 | 22 | 1,471,692 | 7.85% |
		| data/soop/categories/2025/08/23 | 22 | 1,471,677 | 7.85% |
		| data/soop/categories/2025/08/20 | 19 | 1,274,488 | 6.80% |
		| data/soop/categories/2025/08/25 | 10 | 668,869 | 3.57% |
		| . | 8 | 24,167 | 0.13% |
		| .github | 1 | 2,730 | 0.01% |
		| .github/workflows | 1 | 2,730 | 0.01% |
		
		## ğŸŒ³ Depth Distribution
		| Depth | Count |
		| ---: | ---: |
		| 1 | 8 |
		| 3 | 2 |
		| 4 | 1 |
		| 7 | 117 |
		
		## ğŸ§µ Longest Paths (Top 25)
		| Path | Length | Bytes |
		| --- | ---: | ---: |
		| data/soop/categories/categories_master.csv | 42 | 8,624,842 |
		| data/soop/categories/2025/08/20/09.csv | 38 | 66,912 |
		| data/soop/categories/2025/08/20/10.csv | 38 | 66,926 |
		| data/soop/categories/2025/08/20/07.csv | 38 | 67,040 |
		| data/soop/categories/2025/08/20/05.csv | 38 | 70,165 |
		| data/soop/categories/2025/08/20/08.csv | 38 | 66,884 |
		| data/soop/categories/2025/08/20/11.csv | 38 | 66,930 |
		| data/soop/categories/2025/08/20/12.csv | 38 | 66,934 |
		| data/soop/categories/2025/08/20/06.csv | 38 | 67,051 |
		| data/soop/categories/2025/08/20/16.csv | 38 | 66,899 |
		| data/soop/categories/2025/08/20/13.csv | 38 | 66,893 |
		| data/soop/categories/2025/08/20/14.csv | 38 | 66,920 |
		| data/soop/categories/2025/08/20/15.csv | 38 | 66,911 |
		| data/soop/categories/2025/08/20/17.csv | 38 | 66,889 |
		| data/soop/categories/2025/08/20/20.csv | 38 | 66,854 |
		| data/soop/categories/2025/08/20/19.csv | 38 | 66,868 |
		| data/soop/categories/2025/08/20/18.csv | 38 | 66,869 |
		| data/soop/categories/2025/08/20/22.csv | 38 | 66,845 |
		| data/soop/categories/2025/08/20/23.csv | 38 | 66,848 |
		| data/soop/categories/2025/08/21/03.csv | 38 | 66,876 |
		| data/soop/categories/2025/08/21/01.csv | 38 | 66,859 |
		| data/soop/categories/2025/08/21/04.csv | 38 | 66,882 |
		| data/soop/categories/2025/08/21/06.csv | 38 | 66,899 |
		| data/soop/categories/2025/08/21/05.csv | 38 | 66,897 |
		| data/soop/categories/2025/08/21/10.csv | 38 | 66,915 |
		
		## â±ï¸ Temporal
		- Oldest: requirements.txt (2025-08-20T04:44:56.247Z)
		- Newest: .bmadignore (2025-08-25T11:53:46.720Z)
		
		| Age | Files | Bytes |
		| --- | ---: | ---: |
		| > 1 year | 0 | 0 |
		| 6â€“12 months | 0 | 0 |
		| 1â€“6 months | 0 | 0 |
		| 7â€“30 days | 0 | 0 |
		| 1â€“7 days | 7 | 86,522 |
		| < 1 day | 121 | 18,655,328 |
		
		## âœ… Quality Signals
		- Zero-byte files: 0
		- Empty text files: 0
		- Hidden files: 3
		- Symlinks: 0
		- Large files (>= 50 MB): 0
		- Suspiciously large files (>= 100 MB): 0
		
		## ğŸ—œï¸ Compressibility
		Sampled compressibility ratio: 22.56%
		
		## ğŸ”§ Git
		- Tracked: 125 files, 18,731,310 bytes
		- Untracked: 3 files, 10,540 bytes
		
		## ğŸ“š Largest Files (Top 50)
		| Path | Size | % of total | LOC |
		| --- | ---: | ---: | ---: |
		| data/soop/categories/categories_master.csv | 8.2 MB | 46.02% | 57,541 |
		| data/soop/categories_timeseries.csv | 2.2 MB | 12.06% | 51,718 |
		| data/soop/categories/2025/08/20/05.csv | 68.5 KB | 0.37% | 449 |
		| data/soop/categories/2025/08/20/06.csv | 65.5 KB | 0.36% | 449 |
		| data/soop/categories/2025/08/20/07.csv | 65.5 KB | 0.36% | 449 |
		| data/soop/categories/2025/08/22/12.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/12.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/14.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/12.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/13.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/13.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/14.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/12.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/12.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/13.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/13.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/10.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/14.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/14.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/10.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/10.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/09.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/14.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/10.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/09.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/25/11.csv | 65.4 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/15.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/09.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/10.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/15.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/09.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/15.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/06.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/15.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/20/15.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/09.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/16.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/23/17.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/25/10.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/07.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/24/08.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/16.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/08.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/21/16.csv | 65.3 KB | 0.36% | 448 |
		| data/soop/categories/2025/08/22/05.csv | 65.3 KB | 0.36% | 448 |]]></file>
	<file path='package.json'>
		{
		  "devDependencies": {
		    "bmad-method": "^4.40.0"
		  }
		}</file>
	<file path='requirements.txt'>
		requests
		pandas</file>
</files>
